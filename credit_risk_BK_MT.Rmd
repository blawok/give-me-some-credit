---
title: "Credit Risk: Methods of scorecards development in R"
subtitle: 'Credit risk analysis using various methods and creation of scorecard based on the best performing model'
author: "Bartlomiej Kowalczuk & Michal Thor"
date: "27 06 2020"
output: html_document
framework: bootstrap
highlighter: prettify
hitheme: twitter-bootstrap
assets:
  css:
   - "http://fonts.googleapis.com/css?family=Raleway:300"
   - "http://fonts.googleapis.com/css?family=Oxygen"
---
  
  <style>
  body{
    font-family: 'Oxygen', sans-serif;
    font-size: 16px;
    line-height: 24px;
  }

h1,h2,h3,h4 {
  font-family: 'Raleway', sans-serif;
}

.container { width: 1000px; }
h2 {
  background-color: #D4DAEC;
    text-indent: 40px; 
}

g-table-intro h4 {
  text-indent: 0px;
}
</style>
  
<a href="https://github.com/blawok/give-me-some-credit" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#151513; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>

***

## Short description:

In this project we conducted credit risk analysis of database of clients provided on Kaggle. Goal was to predict the probability that somebody will experience financial distress in the next two years. Data is available under this link:

* [Kaggle Competition](https://www.nomisweb.co.uk/query/select/getdatasetbytheme.asp?opt=3&theme=&subgrp=)

If you would like to see the source code for this analysis it is provided in GitHub repository - link in the top right corner.

***
***

# Data preparation

## Data description

Data which we used comes from Kaggle competition and it consists of two initial datasets: training one with 150,000 observations, each with 1 default indicator and 10 numerical features and a test set with a little over 100,000 observations â€“ but without default indicator. The test set will be used for Kaggle competition submission and all the modelling will therefore be done on the (accordingly split) training set.

We also had to change some variable names and remove index variable (for further clarity and smbinning function restrictions).

```{r setup, include=FALSE}

source("load_libraries.R")
```

```{r load_data, echo=FALSE, warning=FALSE, message=FALSE}

data <- read.csv('data/cs-training.csv')

data$def <- data$SeriousDlqin2yrs
data$SeriousDlqin2yrs <- NULL
data$X <- NULL
data$def_two <- (1 - data$def)
data$NumberOfTime30_59DaysPastDueNotWorse <- data$NumberOfTime30.59DaysPastDueNotWorse
data$NumberOfTime60_89DaysPastDueNotWorse <- data$NumberOfTime60.89DaysPastDueNotWorse 
data$NumberOfTime30.59DaysPastDueNotWorse <- NULL
data$NumberOfTime60.89DaysPastDueNotWorse <- NULL
```

*** 

## Data cleaning and split

First, we split the training dataset (150,000 observations) into train/test sets which we will use for training and final evaluation.
We split the data with respect to default - we need to retain percentage of defaults the same in both sets. It is essential, because the dataset is very imbalanced - there are only ~ 7% of defaults in the dataset.
Therefore, after the splitting, we have 0.9/0.1 train/test split - both with around 7% of defaults.

```{r test_split, echo=FALSE, warning=FALSE, message=FALSE}

summary(data$def)

set.seed(361309)
split_var <- sample.split(data$def, SplitRatio = 0.9)
train <- data[split_var==T,]
test <- data[split_var==F,]

```


***

## NA imputation:

```{r na_imputation, echo=FALSE, warning=FALSE, message=FALSE}

# Monthly income NA filling
train$no_income <- ifelse(is.na(train$MonthlyIncome), '1', '0')
test$no_income <- ifelse(is.na(test$MonthlyIncome), '1', '0')

# mean for test set: 6643

train$MonthlyIncome <- na_mean(train$MonthlyIncome)
test$MonthlyIncome <- ifelse(is.na(test$MonthlyIncome), 6643, test$MonthlyIncome)

train$no_dependents <- ifelse(is.na(train$NumberOfDependents), '1', '0')
test$no_dependents <- ifelse(is.na(test$NumberOfDependents), '1', '0')
train$NumberOfDependents <- ifelse(is.na(train$NumberOfDependents), 1, train$NumberOfDependents)
test$NumberOfDependents <- ifelse(is.na(test$NumberOfDependents), 1, test$NumberOfDependents)

```

There are missing values only in two columns: 26716 NAs for *MonthlyIncome* and 3543 for *NumberOfDependents* (which stands for how many people, e.g. family members, depend financially on the analysed customer). We decide to impute mean of *MonthlyIncome* in place of NAs (we use the same mean for test set, so there won't be any data leakage) and create new binary variable indicating that the *MonthlyIncome* is missing. We do the same with *NumberOfDependents* but we use mode instead of mean.

Therefore we end up with 10 numerical variables and 2 factors.

***

## Histograms

Then we have a look at how each variable is distributed using histograms:

```{r histograms, echo=FALSE, warning=FALSE, message=FALSE, fig.width=12, fig.height=6}

numeric_cols <- colnames(select_if(train, is.numeric))[c(1:8)]
hist_df <- gather(train[, which(names(train) %in% numeric_cols)], key = "name", value = "value")

ggplot(hist_df) +
  geom_histogram(aes(value)) +
  facet_wrap(~name, ncol = 4, scales = "free") +
  ggtitle("Histograms of all variables")

rm(hist_df)
```

We can see that some of the variables have outliers which after further inspection might bias our estimations and we decided to impute them with different value.

```{r outliers, echo=FALSE, warning=FALSE, message=FALSE}
remove_outliers <- function(x, na.rm = TRUE) {
  qnt <- quantile(x, probs=c(.05, .95), na.rm = na.rm)
  H <- 2 * IQR(x, na.rm = na.rm)
  y <- x
  y[x < (qnt[1] - H)] <- qnt[1]
  y[x > (qnt[2] + H)] <- qnt[2]
  y
}

train$DebtRatio <- remove_outliers(train$DebtRatio)
train$MonthlyIncome <- remove_outliers(train$MonthlyIncome)
train$RevolvingUtilizationOfUnsecuredLines <- remove_outliers(train$RevolvingUtilizationOfUnsecuredLines)

```

We assess observations that are 2 x IQR under 5th and above 95th percentile to be outliers and we impute them with 5th and 95th percentile respectively. Using this method we changed the value of 3 variables (*RevolvingUtilizationOfUnsecuredLines*, *DebtRatio* and *MonthlyIncome*) and jointly ~8000 values.

***

## Creating WoE variables

```{r woe_sets, echo=FALSE, warning=FALSE, message=FALSE}

train_woe <- as.data.frame(train)
test_woe <- as.data.frame(test)

```

After analyzing the histograms and supporting that with our own experience we decided to split 4 variables using *smbinning* package:

* *NumberOfTime30_59DaysPastDueNotWorse*
* *NumberOfOpenCreditLinesAndLoans*
* *NumberRealEstateLoansOrLines*
* *DebtRatio*

These variables seem to be good candidates - they have low variance but values higher than some threshold might be very indicative.

Below plots and tables for splits are shown. We set the *smbinning* package to split the variable into maximum of 10 intervals.

```{r woe_30_59_pastdue, echo=FALSE, warning=FALSE, message=FALSE, fig.width=12, fig.height=6}

result_1 <- smbinning(train[,c("def_two", "NumberOfTime30_59DaysPastDueNotWorse")], y="def_two", x="NumberOfTime30_59DaysPastDueNotWorse", p=0.1)
train_woe[,"NumberOfTime30_59DaysPastDueNotWorse_coarse"]<- cut(as.numeric(train[,"NumberOfTime30_59DaysPastDueNotWorse"]), breaks=c(-Inf,unique(result_1$cuts),Inf), 
                                     labels=c(paste("<=", unique(result_1$cuts)),"<= Inf"),
                                     include.lowest = T)
test_woe[,"NumberOfTime30_59DaysPastDueNotWorse_coarse"]<- cut(as.numeric(test[,"NumberOfTime30_59DaysPastDueNotWorse"]), breaks=c(-Inf,unique(result_1$cuts),Inf), 
                                    labels=c(paste("<=", unique(result_1$cuts)),"<= Inf"),
                                    include.lowest = T)
IV <- result_1$ivtable
IV$Cutpoint<-ifelse(grepl(">",IV$Cutpoint)==T,"<= Inf",IV$Cutpoint)
train_woe <- merge(train_woe,IV[,c("Cutpoint", "WoE")],by.x= "NumberOfTime30_59DaysPastDueNotWorse_coarse", by.y="Cutpoint", all.x=T, sort=F)
colnames(train_woe)[which(names(train_woe) == "WoE")] <- "NumberOfTime30_59DaysPastDueNotWorse_woe"
test_woe <- merge(test_woe,IV[,c("Cutpoint", "WoE")],by.x= "NumberOfTime30_59DaysPastDueNotWorse_coarse", by.y="Cutpoint", all.x=T, sort=F)
colnames(test_woe)[which(names(test_woe) == "WoE")] <- "NumberOfTime30_59DaysPastDueNotWorse_woe"

plot<-IV[!is.na(IV$WoE) & IV$Cutpoint!="Total",]
plot$pos <- c(0.26, -0.65)
ggplot(plot, aes(x=Cutpoint, fill=WoE)) +
  geom_bar(aes(weight=WoE)) +
  ylab('WoE') +
  ggtitle('NumberOfTime30_59DaysPastDueNotWorse') +
  geom_label(aes(label=paste("WoE=",format(WoE,digits=1)," \n Fill=",round(PctRec,3)*100, "%", sep=""), y=pos), colour = "white", fontface = "bold")

knitr::kable(IV[c(1:2, 4),c(1:4, 9:14)])
```

As we can see, first variable is split into two intervals - equal to zero and above zero. That split is quite intuitive from our experience - people who have been in >30 DPD are more likely to default. Information Value of this variable after splitting is 0.6618 and it visibly splits population into Bads/Goods (hence the higher BadRate in one interval).

```{r woe_num_creditlines, echo=FALSE, warning=FALSE, message=FALSE, fig.width=12, fig.height=6}

result_1 <- smbinning(train[,c("def_two", "NumberOfOpenCreditLinesAndLoans")], y="def_two", x="NumberOfOpenCreditLinesAndLoans", p=0.1)
train_woe[,"NumberOfOpenCreditLinesAndLoans_coarse"]<- cut(as.numeric(train[,"NumberOfOpenCreditLinesAndLoans"]), breaks=c(-Inf,unique(result_1$cuts),Inf), 
                                     labels=c(paste("<=", unique(result_1$cuts)),"<= Inf"),
                                     include.lowest = T)
test_woe[,"NumberOfOpenCreditLinesAndLoans_coarse"]<- cut(as.numeric(test[,"NumberOfOpenCreditLinesAndLoans"]), breaks=c(-Inf,unique(result_1$cuts),Inf), 
                                    labels=c(paste("<=", unique(result_1$cuts)),"<= Inf"),
                                    include.lowest = T)
IV <- result_1$ivtable
IV$Cutpoint<-ifelse(grepl(">",IV$Cutpoint)==T,"<= Inf",IV$Cutpoint)
train_woe <- merge(train_woe,IV[,c("Cutpoint", "WoE")],by.x= "NumberOfOpenCreditLinesAndLoans_coarse", by.y="Cutpoint", all.x=T, sort=F)
colnames(train_woe)[which(names(train_woe) == "WoE")] <- "NumberOfOpenCreditLinesAndLoans_woe"
test_woe <- merge(test_woe,IV[,c("Cutpoint", "WoE")],by.x= "NumberOfOpenCreditLinesAndLoans_coarse", by.y="Cutpoint", all.x=T, sort=F)
colnames(test_woe)[which(names(test_woe) == "WoE")] <- "NumberOfOpenCreditLinesAndLoans_woe"

plot<-IV[!is.na(IV$WoE) & IV$Cutpoint!="Total",]
plot$pos <- c(-0.25, 0.07, -0.02)
ggplot(plot, aes(x=Cutpoint, fill=WoE)) +
  geom_bar(aes(weight=WoE)) +
  ylab('WoE') +
  ggtitle('NumberOfOpenCreditLinesAndLoans')+
  scale_x_discrete(limits = c('<= 3','<= 13','<= Inf')) +
  geom_label(aes(label=paste("WoE=",format(WoE,digits=1)," \n Fill=",round(PctRec,3)*100, "%", sep=""), y=pos), colour = "white", fontface = "bold")

knitr::kable(IV[c(1:3, 5),c(1:4, 9:14)])

```

Second variable, which stands for *Number of open credit lines and loans* was split into 3 intervals - less than 3, between 3 and 13 and above 13. That split is not that logical - people with almost none and people with more than 13 ongoing liabailities are most likely to default (according to our data) but people with between 3 and 13 ongoing loans/credit lines open are rather good clients. IV is 0.66 - again it is considered valuable.

```{r woe_num_realestate, echo=FALSE, warning=FALSE, message=FALSE, fig.width=12, fig.height=6}

result_1 <- smbinning(train[,c("def_two", "NumberRealEstateLoansOrLines")], y="def_two", x="NumberRealEstateLoansOrLines", p=0.1)
train_woe[,"NumberRealEstateLoansOrLines_coarse"]<- cut(as.numeric(train[,"NumberRealEstateLoansOrLines"]), breaks=c(-Inf,unique(result_1$cuts),Inf), 
                                     labels=c(paste("<=", unique(result_1$cuts)),"<= Inf"),
                                     include.lowest = T)
test_woe[,"NumberRealEstateLoansOrLines_coarse"]<- cut(as.numeric(test[,"NumberRealEstateLoansOrLines"]), breaks=c(-Inf,unique(result_1$cuts),Inf), 
                                    labels=c(paste("<=", unique(result_1$cuts)),"<= Inf"),
                                    include.lowest = T)
IV <- result_1$ivtable
IV$Cutpoint<-ifelse(grepl(">",IV$Cutpoint)==T,"<= Inf",IV$Cutpoint)
train_woe <- merge(train_woe,IV[,c("Cutpoint", "WoE")],by.x= "NumberRealEstateLoansOrLines_coarse", by.y="Cutpoint", all.x=T, sort=F)
colnames(train_woe)[which(names(train_woe) == "WoE")] <- "NumberRealEstateLoansOrLines_woe"
test_woe <- merge(test_woe,IV[,c("Cutpoint", "WoE")],by.x= "NumberRealEstateLoansOrLines_coarse", by.y="Cutpoint", all.x=T, sort=F)
colnames(test_woe)[which(names(test_woe) == "WoE")] <- "NumberRealEstateLoansOrLines_woe"

plot<-IV[!is.na(IV$WoE) & IV$Cutpoint!="Total",]
plot$pos <- c(-0.11, 0.13, 0.03)
ggplot(plot, aes(x=Cutpoint, fill=WoE)) +
  geom_bar(aes(weight=WoE)) +
  ylab('WoE') +
  ggtitle('NumberRealEstateLoansOrLines')+
  scale_x_discrete(limits = c('<= 0','<= 1','<= Inf')) +
  geom_label(aes(label=paste("WoE=",format(WoE,digits=1)," \n Fill=",round(PctRec,3)*100, "%", sep=""), y=pos), colour = "white", fontface = "bold")

knitr::kable(IV[c(1:3, 5),c(1:4, 9:14)])
```

Here we can see that clients with 1 mortgage are good clients and no mortgage/more than 1 mortgage indicate that the client is more inclined to default. IV of 0.0466 indicates non-negligeble predictive ability.

```{r woe_debtratio, echo=FALSE, warning=FALSE, message=FALSE, fig.width=12, fig.height=6}

result_1 <- smbinning(train[,c("def_two", "DebtRatio")], y="def_two", x="DebtRatio", p=0.1)
train_woe[,"DebtRatio_coarse"]<- cut(as.numeric(train$DebtRatio), breaks=c(-Inf,unique(result_1$cuts),Inf), 
                                  labels=c(paste("<=", unique(result_1$cuts)),"<= Inf"),
                                  include.lowest = T)
test_woe[,"DebtRatio_coarse"]<- cut(as.numeric(test$DebtRatio), breaks=c(-Inf,unique(result_1$cuts),Inf), 
                                     labels=c(paste("<=", unique(result_1$cuts)),"<= Inf"),
                                     include.lowest = T)
IV <- result_1$ivtable
IV$Cutpoint<-ifelse(grepl(">",IV$Cutpoint)==T,"<= Inf",IV$Cutpoint)
train_woe <- merge(train_woe,IV[,c("Cutpoint", "WoE")],by.x= "DebtRatio_coarse", by.y="Cutpoint", all.x=T, sort=F)
colnames(train_woe)[which(names(train_woe) == "WoE")] <- "DebtRatio_woe"
test_woe <- merge(test_woe,IV[,c("Cutpoint", "WoE")],by.x= "DebtRatio_coarse", by.y="Cutpoint", all.x=T, sort=F)
colnames(test_woe)[which(names(test_woe) == "WoE")] <- "DebtRatio_woe"

plot<-IV[!is.na(IV$WoE) & IV$Cutpoint!="Total",]
plot$pos <- c(0.07, -0.1, -0.3, 0.055, 0.18)
ggplot(plot, aes(x=Cutpoint, fill=WoE)) +
  geom_bar(aes(weight=WoE)) +
  ylab('WoE') +
  ggtitle('DebtRatio')+
  scale_x_discrete(limits = c('<= 0.4234','<= 0.6447','<= 3.9724', '<= 1267', '<= Inf')) +
  geom_label(aes(label=paste("WoE=",format(WoE,digits=1)," \n Fill=",round(PctRec,3)*100, "%", sep=""), y=pos), colour = "white", fontface = "bold")

knitr::kable(IV[c(1:5, 7),c(1:4, 9:14)])
```

Finally the *Debt Ratio* - surprisingly, very high DR indicates that the client is less likely to default according to the data. However it may indicate some data flaws as it is very counter-intuitive. The worst clients are the ones with DR between 0.65 and 3.97. IV of 0.0723 indicates highest predictive ability of all WoE variables.

```{r update_woe_sets, echo=FALSE, warning=FALSE, message=FALSE}
rm(IV)
rm(result_1)

cols_to_woe <-  c("NumberOfTime30_59DaysPastDueNotWorse",
                  "NumberOfOpenCreditLinesAndLoans",
                  "NumberRealEstateLoansOrLines",
                  "DebtRatio")

train_woe <- train_woe[, -which(colnames(train_woe) %in% grep("coarse", colnames(train_woe), value = TRUE) | colnames(train_woe) %in% cols_to_woe) ]
test_woe <- test_woe[, -which(colnames(test_woe) %in% grep("coarse", colnames(test_woe), value = TRUE) | colnames(test_woe) %in% cols_to_woe) ]

# Update vars
train_woe$def <- factor(ifelse(train_woe$def == "1","yes","no"))
train_woe$no_income <- factor(ifelse(train_woe$no_income == "1","yes","no"))
train_woe$no_dependents <- factor(ifelse(train_woe$no_dependents == "1","yes","no"))

test_woe$def <- factor(ifelse(test_woe$def == "1","yes","no"))
test_woe$no_income <- factor(ifelse(test_woe$no_income == "1","yes","no"))
test_woe$no_dependents <- factor(ifelse(test_woe$no_dependents == "1","yes","no"))

train_woe$def_two <- NULL
test_woe$def_two <- NULL
```

***

## Variable correlation 

```{r correlation_mat, echo=FALSE, warning=FALSE, message=FALSE, fig.height=8, fig.width=8}

corr_vars <- train[,which(colnames(train) %in% numeric_cols)]
corr_vars <- corr_vars[,-which(colnames(train) %in% grep("def", colnames(train), value=TRUE))]
res <- round(cor(corr_vars), 2)

col <- colorRampPalette(c("#BB4444", "#EE9988", "#FFFFFF", "#77AADD", "#4477AA"))
corrplot(res, method = "color", col = col(200),
         type = "lower", order = "hclust", number.cex = .7,
         addCoef.col = "black", # Add coefficient of correlation
         tl.col = "black", tl.srt = 90, # Text label color and rotation
         diag = FALSE)


rm(corr_vars)
```

As we can see, there are no significant correlations between any of the variables - therefore we keep all the variables in the further analysis.

***

## Train/Validation split

Now we need to further split the training set into actual training dataset and validation one â€“ we will be optimizing the model using training set for validation set performance. 

We are once again splitting the dataset into 0.9/0.1 train/validation sets.

```{r validation_split, echo=FALSE, warning=FALSE, message=FALSE}

set.seed(361309)
rm(split_var)
split_var <- sample.split(train_woe$def, SplitRatio = 0.9)
train_woe <- train_woe[split_var==T,]
validation_woe <- train_woe[split_var==F,]

```

***

## SMOTE

Since our dataset is very imbalanced (7% of observations are defaults), we try oversampling using SMOTE (Synthetic Minority Oversampling Technique). Using *SMOTE* function from *DMwR* package we are able to double the amount of default observations in our training set (we are not using synthetic observations in the validation set).

```{r smote, echo=TRUE, warning=FALSE, message=FALSE}

# train_woe$ID <- seq.int(nrow(train_woe))
# temp_train <- DMwR::SMOTE(def ~ ., train_woe, perc.over = 50, k = 3)
# temp_train <- subset(temp_train, def == "yes")
# temp_train <- rbind(train_woe, temp_train)
# train_woe_smote <- distinct(temp_train)
# train_woe$ID <- NULL
# train_woe_smote$ID <- NULL
# rm(temp_train)
# 
# summary(train_woe$def)
# summary(train_woe_smote$def)
```

Despite our best tries, adding observations obtained by synthetic oversampling did not improve model's predictive ability in further analysis, so we decided not to use it in this report.

***
***

# Choosing the best logit model

## Data loading

```{r load_sets, echo=TRUE, warning=FALSE, message=FALSE}

rm(list=ls(all=TRUE))

load("data/train_woe.Rdata")
load("data/validation_woe.Rdata")
load("models/logit_model.Rdata")

source("model_functions.R")

```

First, we estimate basic models (with Intercept only and one with all variables) - these will be our benchmarks when assessing the final logit model.

```{r base_models, echo=TRUE, warning=FALSE, message=FALSE}

baza<-glm(def ~ 1,data=train_woe, family=binomial("logit"))

max<-glm(def ~ .,data=train_woe, family=binomial("logit"))

```

Then, we use *stepwise (both)* method to choose only statistically significant variables. We are providing only the code, because the model is previously computed and we loaded it in the previous cell.

```{r load_stepwise, echo=TRUE, warning=FALSE, message=FALSE}

# model_stepwise_both<-step(baza, scope = list(upper=max, lower=baza ), direction = "both", trace=T,steps=30,k=4)
# summary(model_stepwise_both)

```

*** 

## Quality assessment

```{r gof_tests, echo=FALSE, warning=FALSE, message=FALSE}
gf<-pchisq(logit_model$deviance, logit_model$df.residual,lower.tail = F)
paste('Goodness Of Fit test p-value:', gf)

ist<-pchisq(logit_model$null.deviance-logit_model$deviance, logit_model$df.null-logit_model$df.residual,lower.tail = F)
paste('Goodness Of Fit test p-value:', ist)

hr<-hosmerlem(y=train_woe$def, yhat=fitted(logit_model),g=10)
paste('Hosmer-Lemeshow test p-value:', hr$p.value)
```

As we can see model is well fit to the data and the variables are statistically significant (based on GoF, Hosmer-Lemeshow and LR tests p-values).

```{r pd_and_score, echo=FALSE, warning=FALSE, message=FALSE}

# ------------------------------------------------------------------------------ Assigning PD to train_woe 
train_woe$baza<-baza$fitted.values
train_woe$logit_model<-logit_model$fitted.values
train_woe$max<-max$fitted.values

train_woe$logit_model<-predict(logit_model, newdata=train_woe, type="response")

validation_woe$logit_model<-predict(logit_model, newdata=validation_woe, type="response") 
validation_woe$max<-predict(max, newdata=validation_woe, type="response")
validation_woe$baza<-predict(baza, newdata=validation_woe, type="response")

# ------------------------------------------------------------------------------ Score computation (train and test)

validation_woe$score<-(660-40/log(1/2)*log(1/72))+40/log(1/2)*predict(logit_model, newdata=validation_woe, type="link") 
train_woe$score<-(660-40/log(1/2)*log(1/72))+40/log(1/2)*predict(logit_model, newdata=train_woe, type="link") 

```


*** 

## ROC Tests

Now we compare ROC curves of base and max model with our model chosen using stepwise method.

```{r roc_test, echo=FALSE, warning=FALSE, message=FALSE}

# ------------------------------------------------------------------------------ ROC
roc_test_baza<-roc.test(train_woe$def, train_woe$logit_model, train_woe$baza,method="d")$p.value
roc_test_og<-roc.test(train_woe$def, train_woe$max, train_woe$logit_model,method="d")$p.value
paste('Roc test p-value for stepwise and base:', roc_test_baza)
paste('Roc test p-value for stepwise and max:', roc_test_og)
```

P-value of both tests is lower than the significance level (of 0.05) so we reject the null hypotheses in favor of the alternative and conclude that the ROC curves are not equally good.

*** 

## Gini comparison

```{r gini_comparison, echo=FALSE, warning=FALSE, message=FALSE}
paste('Gini coefficients on the validation set')

# MODEL BOTH STEPWISE
paste('Gini coefficient for stepwise model:', 2*auc(validation_woe$def,validation_woe$logit_model,direction="<")-1)
# MODEL ALL VARIABLES
paste('Gini coefficient for max model:', 2*auc(validation_woe$def,validation_woe$max,direction="<")-1)
# MODEL BASE
paste('Gini coefficient for base model:', 2*auc(validation_woe$def,validation_woe$baza,direction="<")-1)

```

As we can see model chosen using stepwise methot outperformed the former - therefore we will be using it in further analysis.

***
***

# Model comparison

## Quick description

In this section we will compare four models build using different methods:

* Logistic Regression (from the previous section)
* Random Forest
* Adaptive Boosting
* Extreme Gradient Boosting

```{r load_models, echo=FALSE, warning=FALSE, message=FALSE}

rm(list=ls(all=TRUE))

load(file = "models/logit_model.Rdata")
load(file = "models/rf_model.Rdata")
load(file = "models/gbm_model.Rdata")
load(file = "models/xgb_model.Rdata")
load(file = "models/result_table.RData")
load(file = "models/train_result_table.RData")
load(file = "models/test_table.RData")
load(file = "models/whole_table.RData")
source("model_functions.R")

```

***

## Logit

Our logit model is the one we obtained using stepwise method, its scores are presented below:

```{r logit_model, echo=FALSE, warning=FALSE, message=FALSE, fig.height=4, fig.width=12}
# ------------------------------------------------------------------------------ Logistic Regression
par(mfrow=c(1,3))

# ROC
plot(roc(result_table$def, result_table$logit_model), print.auc=TRUE, main='ROC Curve')

# histogram of scores
hist(result_table[result_table$def=="no",c("logit_score")], main='Histograms of scores', xlab='Scores')
hist(result_table[result_table$def=="yes",c("logit_score")],add=TRUE,col='Red', alpha=I(0.5))
# legend("topleft",levels(result_table$def), fill=colfill)


# denstiy of scores
sm.density.compare(result_table$logit_score, result_table$def, xlab="Scores from Logit Model")
title(main="Score distribution per default")
colfill<-c(2:(2+length(levels(result_table$def))))
legend("topleft",levels(result_table$def), fill=colfill)

```

***

## Random Forest

Tuning led to the most efficient model with 100 estimators, 5 sampled features at each split and minimal node size being 1.

```{r rf_model, echo=FALSE, warning=FALSE, message=FALSE, fig.height=4, fig.width=12}
# ------------------------------------------------------------------------------ Random Forest
par(mfrow=c(1,3))

# ROC
plot(roc(result_table$def, result_table$rf_model), print.auc=TRUE, main='ROC Curve')

# histogram of scores
hist(result_table[result_table$def=="no",c("rf_score")], main='Histograms of scores', xlab='Scores')
hist(result_table[result_table$def=="yes",c("rf_score")],add=TRUE,col='Red', alpha=I(0.5))
# legend("topleft",levels(result_table$def), fill=colfill)

# denstiy of scores
sm.density.compare(result_table$rf_score, result_table$def, xlab="Scores from Random Forest Model")
title(main="Score distribution per default")
colfill<-c(2:(2+length(levels(result_table$def))))
legend("topleft",levels(result_table$def), fill=colfill)

```

***

## AdaBoost

It turned out that the most optimal set of hyperparameters in AdaBoost model is 80 trees (lesser than in the Random Forest model), learning rate 0.07 and the model sampled 0.7 of the initial set in each tree.

```{r gbm_model, echo=FALSE, warning=FALSE, message=FALSE, fig.height=4, fig.width=12}
# ------------------------------------------------------------------------------ AdaBoost
par(mfrow=c(1,3))

# ROC
plot(roc(result_table$def, result_table$gbm_model), print.auc=TRUE, main='ROC Curve')

# histogram of scores
hist(result_table[result_table$def=="no",c("gbm_score")], main='Histograms of scores', xlab='Scores')
hist(result_table[result_table$def=="yes",c("gbm_score")],add=TRUE,col='Red', alpha=I(0.5))
# legend("topleft",levels(result_table$def), fill=colfill)

# denstiy of scores
sm.density.compare(result_table$gbm_score, result_table$def, xlab="Scores from AdaBoost Model")
title(main="Score distribution per default")
colfill<-c(2:(2+length(levels(result_table$def))))
legend("topleft",levels(result_table$def), fill=colfill)
```

***

## XGBoost Model

Optimal set of hyperparameters for the XGBoost is:

* eta = 0.3
* gamma = 0
* max.depth = 4
* max_delta_step = 0
* subsample = 0.7
* colsample_bytree = 0.85
* lambda = 1
* alpha = 0
* scale_pos_weight = 4
* nrounds = 20


```{r xgb_model, echo=FALSE, warning=FALSE, message=FALSE, fig.height=4, fig.width=12}
# ------------------------------------------------------------------------------ XGBoost
par(mfrow=c(1,3))

# ROC
plot(roc(result_table$def, result_table$xgb_model), print.auc=TRUE, main='ROC Curve')

# histogram of scores
hist(result_table[result_table$def=="no",c("xgb_score")], main='Histograms of scores', xlab='Scores')
hist(result_table[result_table$def=="yes",c("xgb_score")],add=TRUE,col='Red', alpha=I(0.5))
# legend("topleft",levels(result_table$def), fill=colfill)

# denstiy of scores
sm.density.compare(result_table$xgb_score, result_table$def, xlab="Scores from XGBoost Model")
title(main="Score distribution per default")
colfill<-c(2:(2+length(levels(result_table$def))))
legend("topleft",levels(result_table$def), fill=colfill)
```

***

## Comparison tables

All values are computed for validation set:

```{r comparison_table, echo=FALSE, warnings=FALSE, message=FALSE}
load('models/test_comparison_table.RData')
load('models/comparison_table.RData')
par(mfrow=c(1,1))

knitr::kable(comparison_table)

```

As you can see XGBoost outperformes all other methods. Population stability index value is lower than 0.1 so we can conclude that the model is stable (we suspect that in case of Random Forest and AdaBoost models there occured division by zero because a lot of observations had the same value, therefore the outcome is flawed). Considering the Kolmogorov-Smirnov statistics, we can conclude that the distributions of scores in default groups are indeed different.

Below we present Gini coefficients for the test set (never previously seen in the training process):

```{r test_comparison, echo=FALSE, warnings=FALSE, message=FALSE}
knitr::kable(test_comparison)
```

Once again, XGBoost outperforms all other methods, therefore we decide to build the scorecard on its outcomes.

***
***

# Scorecard

## Creating the cutoffs

Once we decided that XGBoost is our go-to model, we use the whole dataset (train+validation+test sets) to divide scores into risk-intervals and therefore create a scorecard.

```{r scorecard, echo=FALSE, warnings=FALSE, message=FALSE, fig.width=12, fig.height=6}

whole_table$def <- as.numeric(ifelse(whole_table$def == "yes", 0, 1))
whole_table$xgb_score<-(660-40/log(1/2)*log(1/72))+40/log(1/2)*log(whole_table$xgb_model)
score_cut <- smbinning(whole_table[,c("def", "xgb_score")], y="def", x="xgb_score", p=0.13)
IV <- score_cut$ivtable
plot<-IV[!is.na(IV$WoE) & IV$Cutpoint!="Total",]
plot$pos <- c(-1, -0.1, 0.45,0.8,1,1.5)
ggplot(plot, aes(x=Cutpoint, fill=WoE)) +
  geom_bar(aes(weight=WoE)) +
  ylab('WoE') +
  ggtitle('WoE Scores Full Set')+
  geom_label(aes(label=paste("WoE=",format(WoE,digits=1)," \n Fill=",round(PctRec,3)*100, "%", sep=""), y=pos), colour = "white", fontface = "bold")

show_table <- IV[1:6,c("Cutpoint", "CntRec", "CntGood", "CntBad", "CntCumGood", "CntCumBad", "GoodRate", "BadRate")]
show_table$Cutpoint <- c("0 - 473.05", "473.05 - 518.35", "518.35 - 565.69", "565.69 - 589.53", "589.53 - 609.47", "609.47 and higher")
show_table$GoodRate <- percent(show_table$GoodRate)
show_table$BadRate <- percent(show_table$BadRate)
colnames(show_table) <- c("Score buckets", "No of clients", "Goods", "Bads", "Cumulative Goods", "Cumulative Bads", "Good rate", "Bad rate")

knitr::kable(show_table)
```
















